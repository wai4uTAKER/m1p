@article{Haber_2017,
	doi = {10.1088/1361-6420/aa9a90},
	url = {https://doi.org/10.1088%2F1361-6420%2Faa9a90},
	year = 2017,
	month = {dec},
	publisher = {{IOP} Publishing},
	volume = {34},
	number = {1},
	pages = {014004},
	author = {Eldad Haber and Lars Ruthotto},
	title = {Stable architectures for deep neural networks},
	journal = {Inverse Problems},
	abstract = {Deep neural networks have become invaluable tools for supervised machine learning, e.g. classification of text or images. While often offering superior results over traditional techniques and successfully expressing complicated patterns in data, deep architectures are known to be challenging to design and train such that they generalize well to new data. Critical issues with deep architectures are numerical instabilities in derivative-based learning algorithms commonly called exploding or vanishing gradients. In this paper, we propose new forward propagation techniques inspired by systems of ordinary differential equations (ODE) that overcome this challenge and lead to well-posed learning problems for arbitrarily deep networks.
The backbone of our approach is our interpretation of deep learning as a parameter estimation problem of nonlinear dynamical systems. Given this formulation, we analyze stability and well-posedness of deep learning and use this new understanding to develop new network architectures. We relate the exploding and vanishing gradient phenomenon to the stability of the discrete ODE and present several strategies for stabilizing deep learning for very deep networks. While our new architectures restrict the solution space, several numerical experiments show their competitiveness with state-of-the-art networks.}
}

@article{katrutsa2015stress,
  title={Stress test procedure for feature selection algorithms},
  author={Katrutsa, AM and Strijov, VV},
  journal={Chemometrics and Intelligent Laboratory Systems},
  volume={142},
  pages={172--183},
  year={2015},
  publisher={Elsevier}
}

@incollection{chernousova2014linear,
  title={Linear regression via Elastic Net: Non-enumerative leave-one-out verification of feature selection},
  author={Chernousova, Elena and Razin, Nikolay and Krasotkina, Olga and Mottl, Vadim and Windridge, David},
  booktitle={Clusters, Orders, and Trees: Methods and Applications},
  pages={377--390},
  year={2014},
  publisher={Springer}
}
@inproceedings{wang2018learning,
  title={Learning credible models},
  author={Wang, Jiaxuan and Oh, Jeeheh and Wang, Haozhu and Wiens, Jenna},
  booktitle={Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  pages={2417--2426},
  year={2018}
}
@article{tibshirani1996regression,
  author =	"Robert J. Tibshirani",
  title =	"Regression Shrinkage and Selection via the Lasso",
  journal =	"Journal of the Royal Statistical Society, Series B",
  year = 	"1996",
  volume =	"58",
  number =	"1",
  pages =	"267--288",
  keywords =	"ridge; lasso",
}

@article{zou2005regularization,
  title={Regularization and variable selection via the elastic net},
  author={Zou, Hui and Hastie, Trevor},
  journal={Journal of the royal statistical society: series B (statistical methodology)},
  volume={67},
  number={2},
  pages={301--320},
  year={2005},
  publisher={Wiley Online Library}
}

@inproceedings{lukas1992methods,
  title={Methods for choosing the regularization parameter},
  author={Lukas, Mark A and others},
  booktitle={Mini Conference on Inverse Problems in Partial Differential Equations},
  pages={89--110},
  year={1992},
  organization={Centre for Mathematics and its Applications, Mathematical Sciences Institute~…}
}
@article{tikhonov1977solutions,
 author =	"A. N. Tikhonov and V. Y. Arsenin",
  year = 	"1977",
  title =	"Solutions of Ill-Posed Problems",
  publisher =	"Winston and Sons",
  address =	"Washington, DC",
  kwds = 	"book, na, ill-posed problem, regularization",
}
@article{svensen2007pattern,
  title =	"Pattern Recognition and Machine Learning",
  author =	"Christopher M. Bishop and Nasser M. Nasrabadi",
  journal =	"J. Electronic Imaging",
  year = 	"2007",
  number =	"4",
  volume =	"16",
  bibdate =	"2017-05-28",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/https://doi.org/10.1117/1.2819119;
		 DBLP,
		 http://dblp.uni-trier.de/db/journals/jei/jei16.html#BishopN07",
  pages =	"049901",
}

@article{potanin2019genetic,
  title={Deep learning neural network structure optimization},
  author={M.S.Potanin, K.O.Vayser, V.A.Zholobov and V.V.Strijov},
  year={2019}
}

@inproceedings{krizhevsky2012imagenet,
  title={Imagenet classification with deep convolutional neural networks},
  author={Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  booktitle={Advances in neural information processing systems},
  pages={1097--1105},
  year={2012}
}

@article{KRAUS2020628,
title = "Deep learning in business analytics and operations research: Models, applications and managerial implications",
journal = "European Journal of Operational Research",
volume = "281",
number = "3",
pages = "628 - 641",
year = "2020",
note = "Featured Cluster: Business Analytics: Defining the field and identifying a research agenda",
issn = "0377-2217",
doi = "https://doi.org/10.1016/j.ejor.2019.09.018",
url = "http://www.sciencedirect.com/science/article/pii/S0377221719307581",
author = "Mathias Kraus and Stefan Feuerriegel and Asil Oztekin",
keywords = "Analytics, Deep learning, Deep neural networks, Managerial implications, Research agenda",
abstract = "Business analytics refers to methods and practices that create value through data for individuals, firms, and organizations. This field is currently experiencing a radical shift due to the advent of deep learning: deep neural networks promise improvements in prediction performance as compared to models from traditional machine learning. However, our research into the existing body of literature reveals a scarcity of research works utilizing deep learning in our discipline. Accordingly, the objectives of this overview article are as follows: (1) we review research on deep learning for business analytics from an operational point of view. (2) We motivate why researchers and practitioners from business analytics should utilize deep neural networks and review potential use cases, necessary requirements, and benefits. (3) We investigate the added value to operations research in different case studies with real data from entrepreneurial undertakings. All such cases demonstrate improvements in operational performance over traditional machine learning and thus direct value gains. (4) We provide guidelines and implications for researchers, managers and practitioners in operations research who want to advance their capabilities for business analytics with regard to deep learning. (5) Our computational experiments find that default, out-of-the-box architectures are often suboptimal and thus highlight the value of customized architectures by proposing a novel deep-embedded network."
}

@article{srivastava2014dropout,
  title =	"Dropout: a simple way to prevent neural networks from
		 overfitting",
  author =	"Nitish Srivastava and Geoffrey E. Hinton and Alex
		 Krizhevsky and Ilya Sutskever and Ruslan
		 Salakhutdinov",
  journal =	"J. Mach. Learn. Res",
  year = 	"2014",
  number =	"1",
  volume =	"15",
  bibdate =	"2019-07-10",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/jmlr/jmlr15.html#SrivastavaHKSS14",
  pages =	"1929--1958",
  URL =  	"http://dl.acm.org/citation.cfm?id=2670313",
}


@article{cortez2009modeling,
  title={Modeling wine preferences by data mining from physicochemical properties},
  author={Cortez, Paulo and Cerdeira, Ant{\'o}nio and Almeida, Fernando and Matos, Telmo and Reis, Jos{\'e}},
  journal={Decision Support Systems},
  volume={47},
  number={4},
  pages={547--553},
  year={2009},
  publisher={Elsevier}
}

@article{automation,
author = {Wistuba, Martin and Rawat, Ambrish and Pedapati, Tejaswini},
title = {Automation of Deep Learning - Theory and Practice},
year = {2020},
isbn = {9781450370875},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
url = {https://doi.org/10.1145/3372278.3390739},
doi = {10.1145/3372278.3390739},
booktitle = {Proceedings of the 2020 International Conference on Multimedia Retrieval},
pages = {5–6},
numpages = {2},
keywords = {neural architecture search, AutoML, deep learning},
location = {Dublin, Ireland},
series = {ICMR ’20}
}

@article{BALDEONCALISTO202076,
 title =	"Self-Adaptive 2D-3D Ensemble of Fully Convolutional
		 Networks for Medical Image Segmentation",
  author =	"Maria G. Baldeon Calisto and Susana K. Lai-Yuen",
  journal =	"CoRR",
  year = 	"2019",
  volume =	"abs/1907.11587",
  bibdate =	"2019-07-30",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/db/journals/corr/corr1907.html#abs-1907-11587",
  URL =  	"http://arxiv.org/abs/1907.11587",
}

@article{CASSIMON2020100234,
title = "Designing Resource-Constrained Neural Networks Using Neural Architecture Search Targeting Embedded Devices",
journal = "Internet of Things",
pages = "100234",
year = "2020",
issn = "2542-6605",
doi = "https://doi.org/10.1016/j.iot.2020.100234",
url = "http://www.sciencedirect.com/science/article/pii/S2542660520300676",
author = "Thomas Cassimon and Simon Vanneste and Stig Bosmans and Siegfried Mercelis and Peter Hellinckx",
keywords = "Neural Architecture Search, Resource Constraint, Embedded Device, Neural Network, Internet of Things"
}

@ARTICLE{9095246,  author={Y. {Fan} and F. {Tian} and Y. {Xia} and T. {Qin} and X. {Li} and T. {Liu}},  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},   title={Searching Better Architectures for Neural Machine Translation},   year={2020},  volume={28},  number={},  pages={1574-1585},}

@ARTICLE{9103245,  author={Y. {Chen} and Z. {Wang} and Z. J. {Wang} and X. {Kang}},  journal={IEEE Journal of Selected Topics in Signal Processing},   title={Automated Design of Neural Network Architectures with Reinforcement Learning for Detection of Global Manipulations},   year={2020},  volume={},  number={},  pages={1-1},}

@unknown{unknown,
author = {Chamberlain, Benjamin and Rowbottom, James and Gorinova, Maria and Webb, Stefan and Rossi, Emanuele and Bronstein, Michael},
year = {2021},
month = {06},
pages = {},
title = {GRAND: Graph Neural Diffusion},
doi = {10.48550/arXiv.2106.10934}
}

@article{macro,
author="Kyriakides, George
and Margaritis, Konstantinos",
editor="Maglogiannis, Ilias
and Iliadis, Lazaros
and Pimenidis, Elias",
title="Regularized Evolution for Macro Neural Architecture Search",
booktitle="Artificial Intelligence Applications and Innovations",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="111--122",
abstract="Neural Architecture Search is becoming an increasingly popular research field and method to design deep learning architectures. Most research focuses on searching for small blocks of deep learning operations, or micro-search. This method yields satisfactory results but demands prior knowledge of the macro architecture's structure. Generally, methods that do not utilize macro structure knowledge perform worse but are able to be applied to datasets of completely new domains. In this paper, we propose a macro NAS methodology which utilizes concepts of Regularized Evolution and Macro Neural Architecture Search (DeepNEAT), and apply it to the Fashion-MNIST dataset. By utilizing our method, we are able to produce networks that outperform other macro NAS methods on the dataset, when the same post-search inference methods are used. Furthermore, we are able to achieve 94.46{\%} test accuracy, while requiring considerably less epochs to fully train our network.",
isbn="978-3-030-49186-4"
}

@article{tikhonov1965,
  author =	"A. N. Tikhonov",
  title =	"Solution of incorrectly formulated problems and the
		 regularization method",
  journal =	"Soviet Math. Dokl.",
  year = 	"1963",
  volume =	"4",
  pages =	"1035--1038",
  keywords =	"regularization, ridge, Tikhonov",
}

@inproceedings{tatarchuk2010support,
  title =	"A Support Kernel Machine for Supervised Selective
		 Combining of Diverse Pattern-Recognition Modalities",
  author =	"Alexander Tatarchuk and Eugene Urlov and Vadim Mottl
		 and David Windridge",
  bibdate =	"2019-10-19",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/https://doi.org/10.1007/978-3-642-12127-2_17;
		 DBLP,
		 http://dblp.uni-trier.de/db/conf/mcs/mcs2010.html#TatarchukUMW10",
  booktitle =	"Multiple Classifier Systems, 9th International
		 Workshop, MCS 2010, Cairo, Egypt, April 7-9, 2010.
		 Proceedings",
  publisher =	"Springer",
  year = 	"2010",
  volume =	"5997",
  booktitle =	"MCS",
  editor =	"Neamat El Gayar and Josef Kittler and Fabio Roli",
  ISBN = 	"978-3-642-12126-5",
  pages =	"165--174",
  series =	"Lecture Notes in Computer Science",
}

@article{vorontsov2015additive,
  title =	"Additive regularization of topic models",
  author =	"Konstantin Vorontsov and Anna Potapenko",
  journal =	"Machine Learning",
  year = 	"2015",
  number =	"1-3",
  volume =	"101",
  bibdate =	"2018-06-26",
  bibsource =	"DBLP,
		 http://dblp.uni-trier.de/https://doi.org/10.1007/s10994-014-5476-6;
		 DBLP,
		 http://dblp.uni-trier.de/db/journals/ml/ml101.html#VorontsovP15",
  pages =	"303--323",
}

@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }


@article{Kolmogorov1956,
  title =	"On the representation of continuous functions of several variables by superpositions of continuous functions of a smaller number of variables",
  author =	"A Kolmogorov",
  journal =	"Amer. Math. Soc. Transl.",
  year = 	"1961",
  number =	"17",
  pages =	"369-373"
}

@article{Cybenko1989,
  title =	"Approximation by Superpositions of a Sigmoidal function",
  author =	"Cybenko, G. V.",
  journal =	"Mathematics of Control Signals and Systems",
  year = 	"1989",
  volume =	"4",
  number =	"4",
  pages =	"303-314"
}

@Article{math7100992,
AUTHOR = {Hanin, Boris},
TITLE = {Universal Function Approximation by Deep Neural Nets with Bounded Width and ReLU Activations},
JOURNAL = {Mathematics},
VOLUME = {7},
YEAR = {2019},
NUMBER = {10},
ARTICLE-NUMBER = {992},
URL = {https://www.mdpi.com/2227-7390/7/10/992},
ISSN = {2227-7390},
ABSTRACT = {This article concerns the expressive power of depth in neural nets with ReLU activations and a bounded width. We are particularly interested in the following questions: What is the minimal width w min ( d ) so that ReLU nets of width w min ( d ) (and arbitrary depth) can approximate any continuous function on the unit cube [ 0 , 1 ] d arbitrarily well? For ReLU nets near this minimal width, what can one say about the depth necessary to approximate a given function? We obtain an essentially complete answer to these questions for convex functions. Our approach is based on the observation that, due to the convexity of the ReLU activation, ReLU nets are particularly well suited to represent convex functions. In particular, we prove that ReLU nets with width d + 1 can approximate any continuous convex function of d variables arbitrarily well. These results then give quantitative depth estimates for the rate of approximation of any continuous scalar function on the d-dimensional cube [ 0 , 1 ] d by ReLU nets with width d + 3 .},
DOI = {10.3390/math7100992}
}

